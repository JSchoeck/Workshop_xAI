{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeMVA5BMb5n5"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahmouddraz/xai/blob/main/notebooks/information_bottelneck/XAI_Information%20Bottleneck%20Theory_exercise.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kr7NiLZZtbZ"
      },
      "source": [
        "# Information Bottleneck Theory\n",
        "\n",
        "## 1. Basic idea\n",
        "\n",
        "The basic idea behind information bottleneck theory is to estimate how much information from one layer of a NN is carried to another. One particular measure one that measures this effect is the  `Mutual Information` (MI). However we need to estimate it, since we a direct computation is too time consuming and in general not known. During the training we can look at how the MI evolves. Of particular interest is the mutual information between the input and some intermediate layer, aswell as the MI between some intermediate layer and the output layer. This evolution can help us understand what happens inside the network during training. \n",
        "\n",
        "## 2. Understanding the Math and Theory\n",
        "\n",
        "### Mutual Information\n",
        "\n",
        "The mutual information (MI) is an important indicator for exploring what happens inside a neural network during training because it shows how the networks develop and if there is compression. In general, it describes the statistical correlation between two random variables. The definition of the mutual information $I (X ; Y )$ is given by \n",
        "$$I(X; Y ) = H(Y ) − H(Y |X)$$\n",
        "where $H(Y)$ is the individual entropy of the output Y (red circle) and $H(Y|X)$ the conditional entropy (red circle minus the orange part).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=195nKURIvl9KOb4oeaJ9sS_3jjNiIzpPW)\n",
        "\n",
        "\n",
        "In neural networks the random variables are neurons or layers of neurons M. On the one hand it is possible to compute the MI of the input layer X which is $I(X;T)$. On the other hand there also is the mutual information from M to the output layer Y $I(Y,T)$. But due to the fact that we don’t know the exact probability distributions of the random variables it is not possible to compute the MI exactly. That is why we need to find some approximation methods.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1s8XfceJEbiQBVFYG_JV0v2y6sszDw6Ng)\n",
        "\n",
        "\n",
        "### Methods to estimate the Mutual Information\n",
        "\n",
        "There are many methods to estimate the mutual information. Some expamples are:\n",
        "* Upper and lower bound\n",
        "* Binning (approximate the MI by discretizising the neuron's output) https://arxiv.org/pdf/1703.00810.pdf \n",
        "* Mutual Information Neural Estimation (MINE) https://arxiv.org/pdf/1801.04062.pdf \n",
        "\n",
        "### Information Bottleneck Theory\n",
        "\n",
        "\n",
        "The information bottleneck bound (IB-bound) is a construct to find the best tradeoff between precision of the prognosis for output Y and a compression of the input X for every layer. To find the optimal IB-bound the mutual information of the output $I(Y;M)$ should be as high as possible for a high precision of the prediction. The mutual information of the input $I(X;M)$ should be as low as possible for a high compression. The best case limit can formally be expressed as \n",
        "$$\\min_{p(t|x),\\, p(y|t),\\, p(t)} I(X;T) - \\beta I(Y;T)$$ \n",
        "where the Lagrange multiplicator β determines the set of relevant information that is kept in the representation $I (Y ; T )$. The minimum of the probability distributions $p$ of the random variables $x, t$ and $y$ has to be found. The families $X, T$ and $Y$ of the random variables follow a Markov chain $Y → X → T$. This implies that $I (X ; Y ) ≥ I(Y;T)$. Which in turn means that the mutual information between input $X$ and output $Y$ is greater than the mutual information between layer $T$ and output $Y$ or the same size.\n",
        "\n",
        "In his work ''Opening the black box of Deep Neural Networks via Information\" Tishby investigated into the influence of different parameters of neural networks on the mutual information and how the mutual information develops during training. His results are based on his understanding of the IB-bound described above. In his experiments Tishby tries to come as close to the IB-bound as possible. His analysis is based on the assumption that a low mutual information of the input I(X;M) stands for compression of the data and leads to well generalized networks. We want this generalization for a higher robustness of the networks, because it improves their performance on noisy data.\n",
        "Tishby discovered two phases that happen during the training of a neural network: the first one is the fitting phase where the network fits the training data. Most of the training epochs are spent on compressing the input, so that the network can represent the data and its relationships more efficiently. Nevertheless Tishby's theory is heavily disputed.\n",
        "\n",
        "\n",
        "Plots of MI can look as follows:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1yicpqdcQvX60pa9SwO96LiRjNPqEpLZP)\n",
        "\n",
        "On the x-axis we can see the MI between the input and layer $M$ wich is the MI of the input. On the y-axis is the MI between the layer $M$ and the output. Because of the decreasing MI of the input in the later epochs, the diagram suggests that the network compresses input data. \n",
        "\n",
        "More information about the Information Bottleneck theory can be found here: https://arxiv.org/pdf/1703.00810.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pRIzSHTwPAj"
      },
      "source": [
        "The MI is related to the \"Kullback-Leibler-divergence\" which measures how much two probability distributions $P, Q$ differ (and is easier to compute/estimate). It is given by \n",
        "$$\n",
        "\\mathrm{KL}(P, Q) = \\sum_{x} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right) .\n",
        "$$\n",
        "Imagine a dice with faces 1 to 6. Consider for example $P$ to be the constant or fair distribution, where every face is equally likely to show up. Then consider different variations where '1' shows up $50\\,\\%$ or $90\\,\\%$ of the time. The KL-divergence will increase, because both differ from the fair distribution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-BJmqxZvFL9"
      },
      "outputs": [],
      "source": [
        "# Computation example for the KL divergence\n",
        "import numpy as np \n",
        "x = [1, 2, 3, 4, 5, 6]\n",
        "P = [1/6] * 6 \n",
        "Q1 = P \n",
        "Q2 = [1/2] + [1/10] * 5 \n",
        "Q3 = [9/10] + [1/50] * 5 \n",
        "\n",
        "KL1 = np.sum([P[i] * np.log(P[i] / Q1[i]) for i in range(len(P))])\n",
        "KL2 = np.sum([P[i] * np.log(P[i] / Q2[i]) for i in range(len(P))])\n",
        "KL3 = np.sum([P[i] * np.log(P[i] / Q3[i]) for i in range(len(P))])\n",
        "print(KL1, KL2, KL3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXhxGJ_oxw_X"
      },
      "source": [
        "\n",
        "## 3. Coding part\n",
        "\n",
        "The notebook is based on https://github.com/artemyk/ibsgd by Artemy Kolchinsky. More notebooks for other data sets such as MNIST can be found there, too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZQvl45DMrem"
      },
      "source": [
        "### Installation and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV_peO7-WC0j"
      },
      "outputs": [],
      "source": [
        "! git clone https://gist.github.com/aishwarya8615/d2107f828d3f904839cbcb7eaa85bd04 'stroke'\n",
        "! git clone https://github.com/artemyk/ibsgd 'MI'\n",
        "! git clone https://github.com/mahmouddraz/xai 'xai_workshop'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4utCBIlMnTz"
      },
      "outputs": [],
      "source": [
        "# Needs to be executed once. Start runtime afterwards.\n",
        "!pip install -r /content/xai_workshop/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTxmUdyZnKin"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import namedtuple\n",
        "from keras import regularizers\n",
        "from keras.layers.core import Dropout\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.activations import relu, sigmoid, softmax, tanh\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sys \n",
        "sys.path.insert(0, \"/content/MI\")\n",
        "import utils\n",
        "import loggingreporter \n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTvFJAXcvIpD"
      },
      "source": [
        "### Prepare Stroke data for the network\n",
        "\n",
        "Before we can train the network we need to load the train and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
        "\n",
        "with open('/content/xai_workshop/pretrained_models/mi_data/MI_trn.pickle', \"rb\") as f:\n",
        "  trn = pickle.load(f)\n",
        "with open('/content/xai_workshop/pretrained_models/mi_data/MI_tst.pickle', \"rb\") as f:\n",
        "  tst = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as4KT94FvNUA"
      },
      "source": [
        "Configure the network settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVhK8mdNvMqm"
      },
      "outputs": [],
      "source": [
        "cfg = {}\n",
        "cfg['SGD_BATCHSIZE']    =     # How many data points in one batch\n",
        "cfg['SGD_LEARNINGRATE'] =     # How fast should the weights be adjusted\n",
        "cfg['NUM_EPOCHS']       =     # Number of epochs\n",
        "cfg['FULL_MI']          = True\n",
        "cfg['ACTIVATION']       =     # Activation function, could be also tanh, softplus or softsign\n",
        "cfg['MAX_EPOCHS']       = cfg['NUM_EPOCHS']\n",
        "cfg['LAYER_DIMS']       =     # Network dimensions (neurons per layer)\n",
        "infoplane_measure        =    # What plot do we want to show (could be upper as well)\n",
        "\n",
        "# Directory to save data during training\n",
        "ARCH_NAME =  '-'.join(map(str,cfg['LAYER_DIMS']))\n",
        "cfg['SAVE_DIR'] = 'rawdata/' + cfg['ACTIVATION'] + '_' + ARCH_NAME "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSJ92jtSTW7N"
      },
      "source": [
        "Create the network based on the settings above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwAeaUCYqBtq"
      },
      "outputs": [],
      "source": [
        "# Input (features of the stroke dataset)\n",
        "input_layer = keras.layers.Input((trn.X.shape[1],))\n",
        "clayer = input_layer\n",
        "for n in cfg['LAYER_DIMS']:\n",
        "    # Add dense layers of the size configured above\n",
        "    clayer = keras.layers.Dense(n, \n",
        "                                activation=cfg['ACTIVATION'],\n",
        "                                kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=1/np.sqrt(float(n)), seed=None),\n",
        "                                bias_initializer='zeros'\n",
        "                               )(clayer)\n",
        "# Output has size 2\n",
        "output_layer = keras.layers.Dense(trn.nb_classes, activation='softmax')(clayer)\n",
        "\n",
        "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Using the adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=cfg['SGD_LEARNINGRATE'])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k11KKiWSUkEb"
      },
      "source": [
        "We don't estimate the MI for every epoch, just selected ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd-QXGMTsPCJ"
      },
      "outputs": [],
      "source": [
        "def do_report(epoch):\n",
        "    if epoch < 30:      \n",
        "        return True\n",
        "    elif epoch < 100:   \n",
        "        return (epoch % 5 == 0)\n",
        "    elif epoch < 1000:  \n",
        "        return (epoch % 20 == 0)\n",
        "    else:     \n",
        "        return (epoch % 100 == 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6WN1iIiXJfv"
      },
      "source": [
        "### Train network and log data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swfdzrQzxDj7"
      },
      "outputs": [],
      "source": [
        "# The reporter is a callback that will be executed for selected epochs\n",
        "reporter = loggingreporter.LoggingReporter(cfg=cfg,                  # Configuration\n",
        "                                          trn=trn,                   # Training data\n",
        "                                          tst=tst,                   # Testing data\n",
        "                                          do_save_func=do_report)    # Estimate for this epoch?\n",
        "# Start the training\n",
        "r = model.fit(x=trn.X, y=trn.Y,                   # Training features data and classes\n",
        "              verbose    = 2,                     # Printing a progress bar while training \n",
        "              batch_size = cfg['SGD_BATCHSIZE'],  # Number of batches\n",
        "              epochs     = cfg['NUM_EPOCHS'],     # Number of epochs\n",
        "              # validation_data=(tst.X, tst.Y),\n",
        "              callbacks  = [reporter,])           # After every epoch, call the reporter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QuOl1Gy3SOM"
      },
      "source": [
        "## 2. Compute MI\n",
        "\n",
        "First, prepare everything for the computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBGD1yZqcxs7"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from __future__ import print_function\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "if not os.path.exists('plots/'):\n",
        "    os.mkdir('plots')\n",
        "\n",
        "from collections import defaultdict, OrderedDict\n",
        "import kde\n",
        "import simplebinmi\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import utils\n",
        "# Functions to estimate the MI\n",
        "import sys \n",
        "sys.path.insert(1, '/content/xai_workshop/notebooks/information_bottelneck')\n",
        "import MI_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPADswH-Uyik"
      },
      "outputs": [],
      "source": [
        "FULL_MI           = cfg['FULL_MI']\n",
        "DO_SAVE           = True              # Save the plot?\n",
        "DO_LOWER          = (infoplane_measure == 'lower')   # Whether to compute lower bounds also\n",
        "DO_BINNED         = (infoplane_measure == 'bin')     # Whether to compute MI estimates based on binning\n",
        "NUM_LABELS        = 2                  # Just two labels: stroke or no stroke\n",
        "COLORBAR_MAX_EPOCHS = cfg['NUM_EPOCHS']# Same as max epochs\n",
        "DIR_TEMPLATE      = '%%s_%s'%ARCH_NAME # Name of the directory (based on the network settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-I8vnpJ55nU"
      },
      "source": [
        "Now we estimate the MI by using the upper bound method and eventually also lower and bin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZizbY4s9R7vM"
      },
      "outputs": [],
      "source": [
        "# Invokes function that computes the MI based on the cofiguration \n",
        "measures, PLOT_LAYERS = MI_utils.compute_MI(cfg, ARCH_NAME, DO_LOWER, DO_BINNED, trn, tst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsYr1q2-6OK5"
      },
      "source": [
        "Finally, we plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TClH1Cq11ll4"
      },
      "outputs": [],
      "source": [
        "MI_utils.print_MI(measures, COLORBAR_MAX_EPOCHS, infoplane_measure, DIR_TEMPLATE, PLOT_LAYERS, ARCH_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfA9Z7iEZvaR"
      },
      "source": [
        "## 4. Exercises\n",
        "\n",
        "### Homework:\n",
        "\n",
        "Please answer the following questions:\n",
        "\n",
        "1. What is the influence of the estimation method on the MI?\n",
        "2. Check the influence of the following hyperparameters on the MI:\n",
        "\n",
        "* Learning Rate \n",
        "* Activation function\n",
        "* Network dimension\n",
        "\n",
        "3. In how far is MI a good concept to understand the training process and the decisions of neural networks?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "XAI_Information Bottleneck Theory_exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
